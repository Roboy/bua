# In addition to this file, load a config file `keys.yml`, where
#  `keys.yml` should contain the following content:
#
#   module: telegramio
#   config:
#     telegram-token: <sexycactus>  # Your telegram token here
#   ---
#   module: ontology
#   config:
#     neo4j_address: bolt://localhost:7687  # Your neo4j server uri here
#     neo4j_username: neo4j                 # Your neo4j user here
#     neo4j_pw: <cornycrab>                 # Your neo4j pw here

---
module: core
config:
  tickrate: 10
  import:
    - ravestate_telegramio
    - ravestate_fillers
    - ravestate_wildtalk
    - ravestate_hibye
    - ravestate_roboyqa
    - ravestate_genqa
    - ravestate_persqa
    #- ravestate_sendpics
    #- ravestate_akinator

---
module: genqa
config:
  drqa_server_address: http://35.246.158.89:5000
  roboy_answer_sanity: 1000

---
module: akinator
config:
  certainty_percentage: 90
---
module: roboyqa
config:
  roboy_node_id: 356
---
module: idle
config:
  impatience_threshold: 6.0  # number of seconds of "pressure" after which a filler should be sent
  bored_threshold: 12.0 # number of seconds of "no activity" after which active engagement should activate
---
module: emotion
config:
  shy_probability: 0.2  # Probability for being shy when input is about Roboy
  surprise_probability: 0.1  # Probability for being surprised when input is question
  happy_probability: 0.1  # Probability for being happy when output is generated
  affectionate_probability: 0.5  # Probability for being affectionate when keyword is in input and input is about Roboy
---
module: wildtalk
config:
  model: "convai_gpt" # one of "convai_gpt", "gpt2", "parlai"
  server_address: "http://localhost"  # can be changed if server is running on its own on a separate machine
  server_port: 5100
  temperature: 0.7  # convai_gpt, gpt2: higher value -> more variation in output
  max_length: 20    # convai_gpt, gpt2: maximal length of generated output
  top_k: 0          # convai_gpt, gpt2: <=0: no filtering, >0: keep only top k tokens with highest probability.
  top_p: 0.9        # convai_gpt: <=0.0 no filtering, >0.0: keep smallest subset whose total probability mass >= top_p
  max_history: 4    # convai_gpt: maximal number of previous dialog turns to be used for output generation
